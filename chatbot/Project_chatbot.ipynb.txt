"""Project_chatbot.ipynb
Automatically generated by Colab.
Original file is located at
    https://colab.research.google.com/drive/1yjHHcubquiNyX2vhlEmH2kWb91auRRtv
"""

# !pip install gradio
# !pip install noisereduce
# !pip install pyloudnorm
# !pip install transformers
# !pip install librosa
# !pip install soundfile

#!/usr/bin/env python
# coding: utf-8

import os
import gradio as gr
import torch
import re
import soundfile as sf
import numpy as np
import librosa
import noisereduce as nr
import pyloudnorm as pyln
from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer, AutoTokenizer, AutoModelForCausalLM

# Load Wav2Vec2 model for audio transcription
model1 = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
tokenizer1 = Wav2Vec2Tokenizer.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

# Load tokenizer and model for text generation
model_id = "microsoft/Phi-3-mini-4k-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ.get('HF_TOKEN'))
model = AutoModelForCausalLM.from_pretrained(model_id, token=os.environ.get('HF_TOKEN'), torch_dtype=torch.bfloat16)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Function to transcribe audio using Wav2Vec2 model
def transcribe_audio(audio_data):
    input_audio = torch.tensor(audio_data).float()
    input_values = tokenizer1(input_audio.squeeze(), return_tensors="pt").input_values
    with torch.no_grad():
        logits = model1(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = tokenizer1.batch_decode(predicted_ids)[0]
    return transcription

# Function to generate response based on transcribed text
def generate_response(transcription):
    try:
        messages = [
            {"role": "system", "content": "You are a chatbot who always responds in english speak!"},
            {"role": "user", "content": transcription},
        ]

        input_ids = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(device)

        terminators = [
            tokenizer.eos_token_id,
            tokenizer.convert_tokens_to_ids("")
        ]

        outputs = model.generate(
            input_ids,
            max_new_tokens=256,
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
        )

        generated_text = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)
        return find_last_sentence(generated_text)

    except Exception as e:
        print("Error during response generation:", e)
        return "Response generation error"

# Function to find last sentence in generated text
def find_last_sentence(text):
    sentence_endings = re.finditer(r'[ред?!]', text)
    end_positions = [ending.end() for ending in sentence_endings]
    if end_positions:
        return text[:end_positions[-1]]
    return text

# Function for spectral subtraction noise reduction
def spectral_subtraction(audio_data, sample_rate):
    stft = librosa.stft(audio_data)
    power_spec = np.abs(stft)**2
    noise_power = np.median(power_spec, axis=1)
    alpha = 2.0
    denoised_spec = np.maximum(power_spec - alpha * noise_power[:, np.newaxis], 0)
    denoised_audio = librosa.istft(np.sqrt(denoised_spec) * np.exp(1j * np.angle(stft)))
    return denoised_audio

# Function for dynamic range compression
def apply_compression(audio_data, sample_rate):
    meter = pyln.Meter(sample_rate)
    loudness = meter.integrated_loudness(audio_data)
    loud_norm = pyln.normalize.loudness(audio_data, loudness, -24.0)
    return loud_norm

# Function to process audio file
def process_audio(audio_file_path):
    try:
        audio_data, sample_rate = librosa.load(audio_file_path)
        print(f"Read audio data: {audio_file_path}, Sample Rate: {sample_rate}")

        reduced_noise = nr.reduce_noise(y=audio_data, sr=sample_rate)
        print("Noise reduction applied")

        denoised_audio = spectral_subtraction(reduced_noise, sample_rate)
        print("Spectral subtraction applied")

        compressed_audio = apply_compression(denoised_audio, sample_rate)
        print("Dynamic range compression applied")

        final_audio = librosa.effects.trim(compressed_audio)[0]
        print("Silences trimmed")

        processed_file_path = 'processed_audio.wav'
        sf.write(processed_file_path, final_audio, sample_rate)
        print(f"Processed audio saved to: {processed_file_path}")

        if not os.path.isfile(processed_file_path):
            raise FileNotFoundError(f"Processed file not found: {processed_file_path}")

        processed_audio_data, _ = librosa.load(processed_file_path, sr=16000)
        print(f"Processed audio reloaded for transcription: {processed_file_path}")

        transcription = transcribe_audio(processed_audio_data)
        print("Transcription completed")

        response = generate_response(transcription)
        print("Response generated")

        return processed_file_path, transcription, response
    except Exception as e:
        print("Error during audio processing:", e)
        return "Error during audio processing", "", ""

# Function to handle text input and generate response
def process_text(text_input):
    try:
        response = generate_response(text_input)
        return "", text_input, response
    except Exception as e:
        print("Error during text processing:", e)
        return "", "Error during text processing", str(e)

# Unified function to handle either audio or text input
def process_input(audio_file_path=None, text_input=None):
    if audio_file_path:
        return process_audio(audio_file_path)
    elif text_input:
        return process_text(text_input)
    else:
        return "No input provided", "", ""

# Create Gradio interface
iface = gr.Interface(
    fn=process_input,
    inputs=[
        gr.Audio(label="Record Audio", type="filepath"),
        gr.Textbox(label="Text Input")
    ],
    outputs=[
        gr.Textbox(label="Processed Audio"),
        gr.Textbox(label="Transcription"),
        gr.Textbox(label="Response")
    ],
    live=False
)

if __name__ == "__main__":
    iface.launch(share=True)


